{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36864bitc9f813a56981486d89d460fa6421c0c5",
   "display_name": "Python 3.6.8 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long short-term memory(LSTM)\n",
    "\n",
    "the algo simply present: **y = activation(dot(state_t, U) + dot(input_t, W) + b)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\nLoading data...\n25000 train sequences\n25000 test sequences\nPad sequences (samples x time)\n"
    }
   ],
   "source": [
    "from keras import layers, models\n",
    "from keras.layers import Embedding\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 500\n",
    "batch_size = 32\n",
    "print('Loading data...')\n",
    "(input_train0, y_train), (input_test0, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(input_train0), 'train sequences')\n",
    "print(len(input_test0), 'test sequences')\n",
    "print('Pad sequences (samples x time)')\n",
    "input_train = sequence.pad_sequences(input_train0, maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(input_test0, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nWARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nTrain on 20000 samples, validate on 5000 samples\nEpoch 1/10\n20000/20000 [==============================] - 40s 2ms/step - loss: 0.4871 - acc: 0.7709 - val_loss: 0.3391 - val_acc: 0.8634\nEpoch 2/10\n20000/20000 [==============================] - 40s 2ms/step - loss: 0.2860 - acc: 0.8890 - val_loss: 0.2770 - val_acc: 0.8880\nEpoch 3/10\n20000/20000 [==============================] - 39s 2ms/step - loss: 0.2277 - acc: 0.9158 - val_loss: 0.2951 - val_acc: 0.8884\nEpoch 4/10\n20000/20000 [==============================] - 39s 2ms/step - loss: 0.1960 - acc: 0.9279 - val_loss: 0.8497 - val_acc: 0.7812\nEpoch 5/10\n20000/20000 [==============================] - 39s 2ms/step - loss: 0.1740 - acc: 0.9387 - val_loss: 0.3216 - val_acc: 0.8770\nEpoch 6/10\n20000/20000 [==============================] - 39s 2ms/step - loss: 0.1577 - acc: 0.9452 - val_loss: 0.3120 - val_acc: 0.8860\nEpoch 7/10\n20000/20000 [==============================] - 39s 2ms/step - loss: 0.1409 - acc: 0.9510 - val_loss: 0.3400 - val_acc: 0.8540\nEpoch 8/10\n20000/20000 [==============================] - 39s 2ms/step - loss: 0.1356 - acc: 0.9516 - val_loss: 0.3149 - val_acc: 0.8814\nEpoch 9/10\n20000/20000 [==============================] - 39s 2ms/step - loss: 0.1198 - acc: 0.9588 - val_loss: 0.3726 - val_acc: 0.8662\nEpoch 10/10\n20000/20000 [==============================] - 39s 2ms/step - loss: 0.1155 - acc: 0.9603 - val_loss: 0.3302 - val_acc: 0.8714\n"
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "history = model.fit(input_train, y_train, epochs=10,batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n['?', 'this', 'film', 'was', 'just', 'brilliant', 'casting', 'location', 'scenery', 'story', 'direction', \"everyone's\", 'really', 'suited', 'the', 'part', 'they', 'played', 'and', 'you', 'could', 'just', 'imagine', 'being', 'there', 'robert', '?', 'is', 'an', 'amazing', 'actor', 'and', 'now', 'the', 'same', 'being', 'director', '?', 'father', 'came', 'from', 'the', 'same', 'scottish', 'island', 'as', 'myself', 'so', 'i', 'loved', 'the', 'fact', 'there', 'was', 'a', 'real', 'connection', 'with', 'this', 'film', 'the', 'witty', 'remarks', 'throughout', 'the', 'film', 'were', 'great', 'it', 'was', 'just', 'brilliant', 'so', 'much', 'that', 'i', 'bought', 'the', 'film', 'as', 'soon', 'as', 'it', 'was', 'released', 'for', '?', 'and', 'would', 'recommend', 'it', 'to', 'everyone', 'to', 'watch', 'and', 'the', 'fly', 'fishing', 'was', 'amazing', 'really', 'cried', 'at', 'the', 'end', 'it', 'was', 'so', 'sad', 'and', 'you', 'know', 'what', 'they', 'say', 'if', 'you', 'cry', 'at', 'a', 'film', 'it', 'must', 'have', 'been', 'good', 'and', 'this', 'definitely', 'was', 'also', '?', 'to', 'the', 'two', 'little', \"boy's\", 'that', 'played', 'the', '?', 'of', 'norman', 'and', 'paul', 'they', 'were', 'just', 'brilliant', 'children', 'are', 'often', 'left', 'out', 'of', 'the', '?', 'list', 'i', 'think', 'because', 'the', 'stars', 'that', 'play', 'them', 'all', 'grown', 'up', 'are', 'such', 'a', 'big', 'profile', 'for', 'the', 'whole', 'film', 'but', 'these', 'children', 'are', 'amazing', 'and', 'should', 'be', 'praised', 'for', 'what', 'they', 'have', 'done', \"don't\", 'you', 'think', 'the', 'whole', 'story', 'was', 'so', 'lovely', 'because', 'it', 'was', 'true', 'and', 'was', \"someone's\", 'life', 'after', 'all', 'that', 'was', 'shared', 'with', 'us', 'all']\n['i', 'love', 'this', 'movie']\n[13, 119, 14, 20]\n['i', 'love', 'this', 'movie']\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0],\n       [1],\n       [1],\n       [0]], dtype=int32)"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "wordIdx = imdb.get_word_index()\n",
    "#wordIdx['a'] = 3\n",
    "wordIdx = {k:(v+3) for k,v in wordIdx.items()}\n",
    "idxWord = {value:key for key,value in wordIdx.items()}\n",
    "#idxWord.get(3) = 'a'\n",
    "#decoded = \" \".join(idxWord[id] for id in input_test0[0] )\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "review = [reverse_word_index.get(i-3, \"?\") for i in input_train0[0]]\n",
    "print(input_train0[0])\n",
    "print(review)\n",
    "\n",
    "data = 'I love this movie'\n",
    "dataArr = data.lower().split()\n",
    "print(dataArr)\n",
    "dataToInd = [word_index.get(i)+3 for i in dataArr]\n",
    "print(dataToInd)\n",
    "dataArr =  [reverse_word_index.get(i-3, \"?\") for i in dataToInd]\n",
    "print(dataArr)\n",
    "\n",
    "model.predict_classes([dataToInd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['i', 'hate', 'this', 'movie'], ['i', 'very', 'love', 'this', 'movie'], ['if', 'i', 'can', 'i', 'will', 'go', 'to', 'see', 'ten', 'times']]\n[[13, 784, 14, 20], [13, 55, 119, 14, 20], [48, 13, 70, 13, 80, 140, 8, 67, 747, 211]]\n[['i', 'hate', 'this', 'movie'], ['i', 'very', 'love', 'this', 'movie'], ['if', 'i', 'can', 'i', 'will', 'go', 'to', 'see', 'ten', 'times']]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.498653  ],\n       [0.55696136],\n       [0.6732189 ],\n       [0.5031713 ],\n       [0.4546673 ]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "datas = ['I hate this movie','I very love this movie', 'if I can I will go to see ten times']\n",
    "dataArr = [data.lower().split() for data in datas]\n",
    "print(dataArr)\n",
    "dataToInd = [[word_index.get(x)+3 for x in i] for i in dataArr]\n",
    "print(dataToInd)\n",
    "dataArr =  [[reverse_word_index.get(x-3, \"?\") for x in i]for i in dataToInd]\n",
    "print(dataArr)\n",
    "\n",
    "model.predict(dataToInd[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}